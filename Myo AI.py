# -*- coding: utf-8 -*-
"""Myo AI Finalized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ydW5fHvKpIbS7RZ374pm5CfCFNiLvJHv

# ğŸ«€ MYO AI: MYOCARDIUM INTELLIGENCE SYSTEM
## A Multi-Layer Cardiac Risk Prediction & Explainability Platform

---

### Project Overview

Myo AI is a modular, end-to-end cardiovascular disease (CVD) prediction system built around **3 architectural layers** and **11 specialized engines**. It ingests heterogeneous clinical & physiological data, trains 5 fully independent ML/DL models in a fair "tournament", and delivers interactive explainability & risk-projection dashboards.

---

### Architecture Summary

| Layer | Codename | Module | Purpose |
|:---:|---|---|---|
| **1 â€” Foundation** | Synapse | `SynapseIngestionEngine` | Downloads & harmonizes 4 disparate clinical CSV datasets |
| | Pulse | `run_pulse_harmonization()` | Chunk-processes ~600 MB ECG time-series â†’ statistical features |
| | Catalyst | `CatalystFeatureSynthesizer` | Merges modalities, engineers BMI / Pulse-Pressure, clips outliers |
| **2 â€” Tournament** | Aegis Protocol | Random Forest Pipeline | Robust tree-ensemble baseline |
| | Myo-Core Engine | HistGradientBoosting Pipeline | Optimized gradient-boosted champion |
| | Sentinel Node | Naive Bayes Pipeline | Probabilistic safety baseline |
| | Vanguard System | Logistic Regression Pipeline | Linear interpretability baseline |
| | Pulse-Sync | 1D-CNN (TensorFlow/Keras) | Deep Learning for complex non-linear patterns |
| **3 â€” Intelligence** | Zenith Map | PCA + KMeans | Unsupervised patient risk-group clustering |
| | Oracle Layer | SHAP + Permutation Importance | Explainable AI â€” global & local feature impact |
| | Myo-Sim Bio-Deck | ipywidgets + Chronos Engine | Interactive digital-twin risk simulator with 20-year projection |

---

### Design Principles

| Principle | Implementation |
|---|---|
| **Strict Model Independence** | Each of the 5 models has its own isolated KDD pipeline: Feature Selection â†’ Train/Test Split â†’ Imputation â†’ Scaling â†’ Training â†’ Evaluation. No model shares preprocessed data or fitted transformers with another. |
| **Fair Tournament** | All models use the same `MASTER_DATA` source and identical split parameters (`test_size=0.2, random_state=42, stratify=y`) so performance is directly comparable. |
| **One Graph Per Cell** | Every visualization cell contains exactly one chart type for clarity and reproducibility. |
| **Colab-Ready** | A dedicated setup cell installs all dependencies; no local environment configuration needed. |
| **Ethical Sensor Handling** | A `sensor_signal_available` flag marks ECG data presence/absence â€” missing sensors are handled transparently, not silently imputed. |

---

### KDD Pipeline (per model)

```
Raw Data â†’ Synapse (Ingest) â†’ Pulse (ECG Features) â†’ Catalyst (Merge & Engineer)
         â†’ MASTER_DATA
         â†’ [Per-Model Independent Pipeline]
            Feature Selection â†’ Stratified Split â†’ Imputation â†’ Scaling â†’ Model Training â†’ Evaluation
```

---

### Technology Stack

| Component | Technology |
|---|---|
| Data Engineering | Pandas, NumPy, SciPy, gdown |
| Machine Learning | Scikit-learn (Pipeline, RandomForest, HGBC, NaiveBayes, LogisticRegression) |
| Deep Learning | TensorFlow / Keras (Conv1D) |
| Explainability | SHAP (TreeExplainer), Permutation Importance |
| Visualization | Matplotlib, Seaborn |
| Interactive UI | ipywidgets |
| Clustering | PCA, KMeans |

---

> **Author**: Ahmed  
> **Runtime**: Google Colab (GPU not required)  
> **Python**: 3.10+
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  GOOGLE COLAB â€” Dependency Installation (Run Once)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

!pip install -q gdown shap tensorflow scikit-learn scipy pandas numpy seaborn matplotlib ipywidgets
print("âœ… All dependencies installed.")

"""### ğŸ“¦ Global Imports

| Category | Libraries | Purpose |
|---|---|---|
| **Data** | `pandas`, `numpy`, `scipy`, `gdown` | Data manipulation, numerical ops, statistical functions, Google Drive downloads |
| **Visualization** | `matplotlib`, `seaborn` | Static plots, heatmaps, styled charts |
| **ML** | `sklearn` (Pipeline, Imputer, Scaler, RF, HGBC, NB, LogReg, PCA, KMeans) | Preprocessing, classification, clustering |
| **DL** | `tensorflow`, `keras` | Conv1D neural network (Pulse-Sync) |
| **XAI** | `shap` | Explainable AI â€” SHAP TreeExplainer |
| **UI** | `ipywidgets` | Interactive patient simulator dashboard |
"""

# ==============================================================================
# ğŸ«€ MYO AI â€” GLOBAL IMPORTS (Run Once)
# ==============================================================================

import os
import time
import warnings
import gdown
import shap
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import ipywidgets as widgets
from IPython.display import display, clear_output
from scipy.stats import skew, kurtosis

# Machine Learning
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.inspection import permutation_importance

# Deep Learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Configuration
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
print("âœ… MYO AI: System Dependencies Loaded.")

"""# ğŸ§¬ LAYER 1 â€” THE FOUNDATION (Data Engineering)

> Ingests 4 heterogeneous clinical datasets, extracts ECG statistical features, and synthesizes a unified `MASTER_DATA` DataFrame.

| # | Engine | Class / Function | Input | Output | Key Technique |
|:---:|---|---|---|---|---|
| 1 | **Synapse** | `SynapseIngestionEngine` | 4 Google Drive CSVs | `df_tabular` (harmonized rows) | gdown download, column renaming, outer concat |
| 2 | **Pulse** | `run_pulse_harmonization()` | `ecg_timeseries.csv` (~600 MB) | `df_ecg_features` (mean, std, skew, kurtosis) | Chunk-based streaming, scipy stats |
| 3 | **Catalyst** | `CatalystFeatureSynthesizer` | `df_tabular` + `df_ecg_features` | `MASTER_DATA` | Left merge, BMI / Pulse-Pressure engineering, BP clipping |

### ğŸ”Œ Synapse Ingestion Engine

| Property | Detail |
|---|---|
| **Purpose** | Download and harmonize 4 disparate cardiac CSV datasets into one unified DataFrame |
| **Datasets** | Heart Attack (Kaggle), Cardiac Failure (Kaggle), ECG Time-Series, Cardiac Failure Base (semicolon-delimited) |
| **Method** | Google Drive download via `gdown`, column renaming to canonical schema, source provenance tagging, outer-join concatenation |
| **Output** | `df_tabular` â€” a single harmonized DataFrame with all patient rows and a `source` column |
| **Key Design** | Idempotent downloads (skips if file exists), string-safe `id` column, outer join preserves all columns across sources |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SYNAPSE INGESTION ENGINE â€” Multi-Source Clinical Data Loader
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import pandas as pd
import numpy as np
import gdown
import os


class SynapseIngestionEngine:
    """
    Downloads and harmonizes heterogeneous cardiac datasets into a
    single analysis-ready tabular DataFrame.

    Datasources
    -----------
    - Heart Attack (Kaggle)
    - Cardiac Failure (Kaggle)
    - Cardiac Failure Base (semicolon-delimited)
    """

    FILE_IDS = {
        'heart_attack':          '1mopCa200spbeFRFpkr_ppuLbqsdl0BaR',
        'cardiac_failure':       '1NBL96uw95T5nhH2_bncc-jvVEOp6_Mjn',
        'ecg_timeseries':        '1MFOoFkk_ypdbH2jvPXU7YWiNDlvPd-en',
        'cardiac_failure_base':  '1_pcIRUWpHoUNkiHcDK01HlLkOOMXi9hn',
    }

    RENAME_MAP = {
        'output': 'target',
        'DEATH_EVENT': 'target',
        'label': 'target',
        'trtbps': 'sys_bp',
        'high_blood_pressure': 'sys_bp',
        'chol': 'cholesterol',
        'serum_cholesterol': 'cholesterol',
    }

    # â”€â”€ Download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def download_data(self) -> dict:
        """Download all CSVs from Google Drive if not already cached."""
        paths = {}
        for name, f_id in self.FILE_IDS.items():
            output = f'{name}.csv'
            if not os.path.exists(output):
                print(f"  â†“  Downloading {name}...")
                gdown.download(
                    f'https://drive.google.com/uc?id={f_id}',
                    output,
                    quiet=False,
                )
            paths[name] = output
        print("âœ… Synapse Download Complete.")
        return paths

    # â”€â”€ Ingest & Harmonize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def ingest_and_harmonize(self, paths: dict) -> pd.DataFrame:
        """
        Read the 3 tabular CSVs, rename columns to a canonical
        schema, tag with source, and merge into one DataFrame.
        """
        df_ha = pd.read_csv(paths['heart_attack'])
        df_cf = pd.read_csv(paths['cardiac_failure'])
        df_ex = pd.read_csv(paths['cardiac_failure_base'], sep=';')

        # Canonical column names
        df_ha = df_ha.rename(columns=self.RENAME_MAP)
        df_cf = df_cf.rename(columns=self.RENAME_MAP)
        df_ex = df_ex.rename(columns=self.RENAME_MAP)

        # Source provenance tags
        df_ha['source'] = 'HeartAttack'
        df_cf['source'] = 'CardiacFailure'
        df_ex['source'] = 'CardiacFailureBase'

        # Ensure 'id' is string before concat
        for df in (df_cf, df_ex):
            if 'id' in df.columns:
                df['id'] = df['id'].astype(str)

        # Union merge (outer join keeps all columns)
        df_tabular = pd.concat(
            [df_ha, df_cf, df_ex],
            axis=0,
            ignore_index=True,
            join='outer',
        )

        if 'id' in df_tabular.columns:
            df_tabular['id'] = df_tabular['id'].astype(str)

        print(f"âœ… Synapse Harmonization Complete  â†’  {len(df_tabular):,} patient rows")
        return df_tabular


# â”€â”€ Instantiate & Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
synapse = SynapseIngestionEngine()
paths   = synapse.download_data()
df_tabular = synapse.ingest_and_harmonize(paths)

"""### âš¡ Pulse-Harmonization Engine

| Property | Detail |
|---|---|
| **Purpose** | Stream-process a large ECG time-series CSV and extract per-patient statistical features |
| **Input** | `ecg_timeseries.csv` (potentially ~600 MB) |
| **Output** | `df_ecg_features` â€” DataFrame with columns: `id`, `ecg_mean`, `ecg_std`, `ecg_skew`, `ecg_kurtosis`, `source` |
| **Technique** | Chunk-based reading (`chunksize=100,000`), auto-detection of signal column, `scipy.stats.skew` & `kurtosis` |
| **Memory Safety** | Never loads entire file into RAM; processes in configurable chunks and aggregates |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PULSE-HARMONIZATION ENGINE â€” ECG Time-Series Feature Extractor
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from scipy.stats import skew, kurtosis


def _extract_ecg_stats(group: pd.Series) -> pd.Series:
    """Compute 4 statistical moments from raw ECG amplitude values."""
    data = pd.to_numeric(group, errors='coerce').dropna()

    if len(data) == 0:
        return pd.Series({
            'ecg_mean': 0, 'ecg_std': 0,
            'ecg_skew': 0, 'ecg_kurtosis': 0,
        })

    return pd.Series({
        'ecg_mean':     data.mean(),
        'ecg_std':      data.std()      if len(data) >= 2 else 0,
        'ecg_skew':     skew(data)      if len(data) >= 3 else 0,
        'ecg_kurtosis': kurtosis(data)  if len(data) >= 3 else 0,
    })


def run_pulse_harmonization(file_path: str, chunk_size: int = 100_000) -> pd.DataFrame:
    """
    Stream-process a large ECG CSV in chunks, extracting per-patient
    statistical features (mean, std, skew, kurtosis).

    Parameters
    ----------
    file_path : str
        Path to `ecg_timeseries.csv`.
    chunk_size : int
        Rows per chunk (default 100 000).

    Returns
    -------
    pd.DataFrame
        Columns: id | ecg_mean | ecg_std | ecg_skew | ecg_kurtosis | source
    """
    print("âš¡ Pulse-Harmonization: Processing ECG signal file...")

    # 1. Detect signal column automatically
    header = pd.read_csv(file_path, nrows=2)
    all_cols = header.columns.tolist()
    print(f"   Columns detected: {all_cols}")

    sig_candidates = [
        c for c in all_cols
        if c.isdigit() or 'sig' in c.lower() or 'val' in c.lower()
    ]
    if '0' in sig_candidates:
        sig_col = '0'
    elif sig_candidates:
        sig_col = sig_candidates[0]
    else:
        sig_col = all_cols[1]

    print(f"   Signal column: '{sig_col}'")

    # 2. Chunk-wise feature extraction
    ecg_feature_list = []
    global_row_offset = 0

    for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):
        chunk['id'] = (chunk.index + global_row_offset).astype(str)
        global_row_offset += len(chunk)

        feats = chunk.groupby('id')[sig_col].apply(_extract_ecg_stats).unstack()
        ecg_feature_list.append(feats)

    # 3. Aggregate across chunks
    df_ecg_features = pd.concat(ecg_feature_list).groupby(level=0).mean()
    df_ecg_features['source'] = 'ECG_Signal'
    df_ecg_features.index.name = 'id'
    df_ecg_features = df_ecg_features.reset_index()

    print(f"âœ… Pulse-Harmonization Complete  â†’  {len(df_ecg_features):,} ECG records")
    return df_ecg_features


# â”€â”€ Execute â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df_ecg_features = run_pulse_harmonization(paths['ecg_timeseries'])

"""### ğŸ§ª Catalyst Feature Synthesizer

| Property | Detail |
|---|---|
| **Purpose** | Merge tabular clinical data with ECG statistical features and engineer clinically meaningful derived variables |
| **Inputs** | `df_tabular` (Synapse output), `df_ecg_features` (Pulse output) |
| **Output** | `MASTER_DATA` â€” the analysis-ready master DataFrame used by all downstream models |
| **Engineered Features** | `bmi` (weight / heightÂ²), `pulse_pressure` (systolic âˆ’ diastolic), `sensor_signal_available` (ECG presence flag) |
| **Outlier Handling** | Systolic BP clipped to [80, 200], Diastolic BP clipped to [50, 120] |
| **Ethical Design** | `sensor_signal_available` flag transparently marks missing ECG data instead of silently imputing |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  CATALYST FEATURE SYNTHESIZER â€” Multimodal Feature Engineering
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import pandas as pd
import numpy as np


class CatalystFeatureSynthesizer:
    """
    Merges tabular clinical data with ECG statistical features and
    synthesizes clinically meaningful derived variables.

    Pipeline
    --------
    1. Left-merge tabular + ECG on patient `id`
    2. Create `sensor_signal_available` missingness flag
    3. Clip blood-pressure outliers to physiological ranges
    4. Engineer BMI and Pulse Pressure
    5. Canonicalize the target column
    """

    # Physiological clipping ranges
    BP_SYSTOLIC_RANGE  = (80, 200)   # ap_hi
    BP_DIASTOLIC_RANGE = (50, 120)   # ap_lo

    # Possible names for the binary target across datasets
    TARGET_CANDIDATES = ['cardio', 'heartdisease', 'output', 'target']

    def synthesize(
        self,
        df_tab: pd.DataFrame,
        df_ecg: pd.DataFrame,
    ) -> pd.DataFrame:
        """
        Execute the full Catalyst pipeline and return MASTER_DATA.

        Parameters
        ----------
        df_tab : pd.DataFrame   Harmonized tabular patient data.
        df_ecg : pd.DataFrame   ECG statistical features (from Pulse engine).

        Returns
        -------
        pd.DataFrame  Analysis-ready MASTER_DATA with all derived features.
        """
        # â”€â”€ 1. Standardize IDs & merge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        df_tab  = df_tab.copy()
        df_ecg  = df_ecg.copy()
        df_tab['id']  = df_tab['id'].astype(str).str.strip()
        df_ecg['id']  = df_ecg['id'].astype(str).str.strip()

        df = pd.merge(df_tab, df_ecg, on='id', how='left').reset_index(drop=True)
        df.columns = df.columns.str.lower()
        print(f"   Merged shape: {df.shape}")

        # â”€â”€ 2. Sensor-availability flag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        df['sensor_signal_available'] = df['ecg_mean'].notnull().astype(int)
        avail = df['sensor_signal_available'].sum()
        print(f"   ECG signal present for {avail:,} / {len(df):,} patients")

        # â”€â”€ 3. Blood-pressure outlier clipping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if 'ap_hi' in df.columns:
            df['ap_hi'] = df['ap_hi'].clip(*self.BP_SYSTOLIC_RANGE)
        if 'ap_lo' in df.columns:
            df['ap_lo'] = df['ap_lo'].clip(*self.BP_DIASTOLIC_RANGE)

        # â”€â”€ 4. Clinical feature engineering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if 'weight' in df.columns and 'height' in df.columns:
            df['bmi'] = df['weight'] / ((df['height'] / 100) ** 2)
            print("   âœ“ BMI synthesized  (weight / heightÂ²)")

        if 'ap_hi' in df.columns and 'ap_lo' in df.columns:
            df['pulse_pressure'] = df['ap_hi'] - df['ap_lo']
            print("   âœ“ Pulse Pressure synthesized  (systolic âˆ’ diastolic)")

        # â”€â”€ 5. Canonicalize target column â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for candidate in self.TARGET_CANDIDATES:
            if candidate in df.columns:
                df = df.rename(columns={candidate: 'target'})
                break

        print(f"âœ… Catalyst Synthesis Complete  â†’  {df.shape[1]} features, "
              f"{len(df):,} rows")
        return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  EXECUTION â€” Build MASTER_DATA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

catalyst    = CatalystFeatureSynthesizer()
MASTER_DATA = catalyst.synthesize(df_tabular, df_ecg_features)

# Quick sanity check
print("\nâ”€â”€ MASTER_DATA Preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"   Shape      : {MASTER_DATA.shape}")
print(f"   Target dist:\n{MASTER_DATA['target'].value_counts().to_string()}")
print(f"   Null %      :\n{(MASTER_DATA.isnull().mean() * 100).nlargest(5).round(1).to_string()}")
MASTER_DATA.head(3)

"""# âš”ï¸ LAYER 2 â€” THE TOURNAMENT (Model Training & Evaluation)

> A "Battle Royale" between 5 fully independent ML/DL architectures. Each model has its own **isolated KDD pipeline**: Feature Selection â†’ Train/Test Split â†’ Imputation â†’ Scaling â†’ Training â†’ Evaluation. No model shares preprocessed data or fitted transformers with another.

| # | Codename | Architecture | Independent Pipeline | Key Hyperparameters |
|:---:|---|---|---|---|
| 1 | **Aegis Protocol** | Random Forest | Imputer â†’ RandomForest | `n_estimators=100`, `max_depth=12` |
| 2 | **Myo-Core Engine** | HistGradientBoosting | Imputer â†’ Scaler â†’ HGBC | `max_iter=300`, `lr=0.05`, `L2=1.5` |
| 3 | **Sentinel Node** | Naive Bayes | Imputer â†’ MinMaxScaler â†’ GaussianNB | Non-parametric |
| 4 | **Vanguard System** | Logistic Regression | Imputer â†’ Scaler â†’ LogReg | `max_iter=1000` |
| 5 | **Pulse-Sync** | 1D-CNN (Keras) | Imputer â†’ Scaler â†’ Conv1D â†’ Dense | `epochs=10`, `filters=64/32`, `dropout=0.3` |

### Strict Independence Guarantee
- Each model cell independently: selects features from `MASTER_DATA`, performs its own stratified split, fits its own imputer/scaler, trains, and evaluates
- Split parameters are identical (`test_size=0.2`, `random_state=42`, `stratify=y`) for fair comparison
- Results are collected into `tournament_results` for the final leaderboard

### ğŸ—„ï¸ Tournament Storage Initialization

| Property | Detail |
|---|---|
| **Purpose** | Initialize empty containers that each independent model cell will append its results to |
| **Containers** | `tournament_results` (metrics list), `tournament_predictions` (y_pred dict), `tournament_probabilities` (y_prob dict) |
| **Design** | Acts as a shared scoreboard only â€” no preprocessing or data is shared between models |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TOURNAMENT STORAGE â€” Shared Scoreboard (No Data Shared)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Each model will independently append its metrics here
tournament_results       = []   # List of dicts: Model, Accuracy, ROC-AUC, Train Time
tournament_predictions   = {}   # model_name â†’ y_pred array
tournament_probabilities = {}   # model_name â†’ y_prob array
tournament_y_test        = None # Will be set by first model (identical across all)

print("âœ… Tournament scoreboard initialized â€” ready for independent model cells.")

"""### ğŸ›¡ï¸ Aegis Protocol â€” Independent Random Forest (1/5)

| Property | Detail |
|---|---|
| **Purpose** | Establish a robust baseline for cardiovascular risk prediction using an ensemble of decision trees |
| **Input Data** | `MASTER_DATA` (Numeric subsets), filtered to remove ID/leakage columns (`aegis_X`, `aegis_y`) |
| **Pipeline Architecture** | `SimpleImputer` (Strategy: Median) â†’ `RandomForestClassifier` |
| **Hyperparameters** | `n_estimators=100`, `max_depth=12`, `n_jobs=-1` (Parallel processing) |
| **Split Strategy** | Independent Stratified 80/20 Split (`test_size=0.2`, `random_state=42`) |
| **Output** | Accuracy & ROC-AUC metrics appended to `tournament_results` dictionary |
| **Independence** | Uses its own isolated `train_test_split` to prevent data leakage from other models |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  AEGIS PROTOCOL â€” Independent Random Forest Pipeline (1/5)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Independent Feature Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
aegis_X_raw = MASTER_DATA.select_dtypes(include=[np.number])
aegis_cols_drop = ['target', 'id', 'unnamed: 0', 'patient_id']
aegis_X = aegis_X_raw.drop(columns=[c for c in aegis_cols_drop if c in aegis_X_raw.columns])
aegis_y = MASTER_DATA['target'].fillna(0).astype(int)

# â”€â”€ 2. Independent Stratified Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
aegis_X_train, aegis_X_test, aegis_y_train, aegis_y_test = train_test_split(
    aegis_X, aegis_y, test_size=0.2, random_state=42, stratify=aegis_y,
)

# â”€â”€ 3. Independent Pipeline (Imputer â†’ RF) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
aegis_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('clf',     RandomForestClassifier(
                    n_estimators=100,
                    max_depth=12,
                    random_state=42,
                    n_jobs=-1)),
])

# â”€â”€ 4. Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t0 = time.time()
aegis_pipeline.fit(aegis_X_train, aegis_y_train)
aegis_elapsed = time.time() - t0

# â”€â”€ 5. Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
aegis_y_pred = aegis_pipeline.predict(aegis_X_test)
aegis_y_prob = aegis_pipeline.predict_proba(aegis_X_test)[:, 1]
aegis_acc = accuracy_score(aegis_y_test, aegis_y_pred)
aegis_auc = roc_auc_score(aegis_y_test, aegis_y_prob)

# â”€â”€ 6. Store in Tournament â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tournament_results.append({
    'Model': 'Aegis Protocol (RF)',
    'Accuracy': aegis_acc,
    'ROC-AUC': aegis_auc,
    'Train Time (s)': round(aegis_elapsed, 2),
})
tournament_predictions['Aegis Protocol\n(Random Forest)'] = aegis_y_pred
tournament_probabilities['Aegis Protocol (RF)'] = aegis_y_prob
tournament_y_test = aegis_y_test

print(f"  âœ“ Aegis Protocol (RF)            Acc={aegis_acc:.4f}  AUC={aegis_auc:.4f}  ({aegis_elapsed:.1f}s)")
print(f"    Dataset: {aegis_X.shape[0]:,} patients Ã— {aegis_X.shape[1]} features")
print(f"    Train: {len(aegis_X_train):,}  Test: {len(aegis_X_test):,}")
print("âœ… Aegis Protocol â€” Independent pipeline complete (1/5)")

"""### âš¡ Myo-Core Engine â€” HistGradientBoosting (2/5)

| Property | Detail |
|---|---|
| **Purpose** | High-performance gradient boosting optimized for speed and accuracy on large datasets |
| **Pipeline Architecture** | `SimpleImputer` (Median) â†’ `StandardScaler` â†’ `HistGradientBoostingClassifier` |
| **Key Hyperparameters** | `learning_rate=0.05`, `max_iter=300`, `max_depth=12`, `l2_regularization=1.5` |
| **Output** | The "Champion" model candidate; typically achieves highest ROC-AUC |
| **Special Role** | This model's pipeline components (`imputer`, `scaler`) are **exported** to be used as the pre-processor for the Deep Learning (Pulse-Sync) and SHAP (Oracle) layers |
| **Independence** | Uses its own isolated `train_test_split` to ensure zero data leakage |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MYO-CORE ENGINE â€” Independent HistGradientBoosting Pipeline (2/5)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Independent Feature Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
myocore_X_raw = MASTER_DATA.select_dtypes(include=[np.number])
myocore_cols_drop = ['target', 'id', 'unnamed: 0', 'patient_id']
myocore_X = myocore_X_raw.drop(columns=[c for c in myocore_cols_drop if c in myocore_X_raw.columns])
myocore_y = MASTER_DATA['target'].fillna(0).astype(int)
myocore_feature_names = myocore_X.columns.tolist()

# â”€â”€ 2. Independent Stratified Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
myocore_X_train_raw, myocore_X_test_raw, myocore_y_train, myocore_y_test = train_test_split(
    myocore_X, myocore_y, test_size=0.2, random_state=42, stratify=myocore_y,
)

# â”€â”€ 3. Independent Pipeline (Imputer â†’ Scaler â†’ HGBC) â”€â”€â”€â”€â”€â”€â”€â”€
myocore_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler',  StandardScaler()),
    ('clf',     HistGradientBoostingClassifier(
                    max_iter=300,
                    learning_rate=0.05,
                    max_depth=12,
                    l2_regularization=1.5,
                    random_state=42)),
])

# â”€â”€ 4. Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t0 = time.time()
myocore_pipeline.fit(myocore_X_train_raw, myocore_y_train)
myocore_elapsed = time.time() - t0

# â”€â”€ 5. Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
myocore_y_pred = myocore_pipeline.predict(myocore_X_test_raw)
myocore_y_prob = myocore_pipeline.predict_proba(myocore_X_test_raw)[:, 1]
myocore_acc = accuracy_score(myocore_y_test, myocore_y_pred)
myocore_auc = roc_auc_score(myocore_y_test, myocore_y_prob)

# â”€â”€ 6. Store in Tournament â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tournament_results.append({
    'Model': 'Myo-Core Engine (HGBC)',
    'Accuracy': myocore_acc,
    'ROC-AUC': myocore_auc,
    'Train Time (s)': round(myocore_elapsed, 2),
})
tournament_predictions['Myo-Core Engine\n(HGBC)'] = myocore_y_pred
tournament_probabilities['Myo-Core Engine (HGBC)'] = myocore_y_prob

# â”€â”€ 7. Export for Layer 3 (Zenith, Oracle, Myo-Sim) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
myocore_imputer = myocore_pipeline.named_steps['imputer']
myocore_scaler  = myocore_pipeline.named_steps['scaler']
myocore_model   = myocore_pipeline.named_steps['clf']
myocore_X_train = myocore_pipeline[:-1].transform(myocore_X_train_raw)
myocore_X_test  = myocore_pipeline[:-1].transform(myocore_X_test_raw)

print(f"  âœ“ Myo-Core Engine (HGBC)         Acc={myocore_acc:.4f}  AUC={myocore_auc:.4f}  ({myocore_elapsed:.1f}s)")
print(f"    Dataset: {myocore_X.shape[0]:,} patients Ã— {myocore_X.shape[1]} features")
print(f"    Train: {len(myocore_X_train_raw):,}  Test: {len(myocore_X_test_raw):,}")
print("âœ… Myo-Core Engine â€” Independent pipeline complete (2/5)")
print("   â†³ Exported: myocore_imputer, myocore_scaler, myocore_model for Layer 3")

"""### ğŸ‘ï¸ Sentinel Node â€” Naive Bayes (3/5)

| Property | Detail |
|---|---|
| **Purpose** | A fast, probabilistic baseline that assumes feature independence (Gaussian Naive Bayes) |
| **Pipeline Architecture** | `SimpleImputer` (Median) â†’ `MinMaxScaler` â†’ `GaussianNB` |
| **Scaling Strategy** | Uses `MinMaxScaler` (0-1 range) instead of Standard scaling, accommodating the probabilistic nature of the model |
| **Role** | Acts as a "sanity check" â€” if complex models (like RF or HGBC) can't beat this simple probabilistic approach, they are likely overfitting |
| **Independence** | Maintains strict isolation with its own `train_test_split` to prevent data leakage |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SENTINEL NODE â€” Independent Naive Bayes Pipeline (3/5)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Independent Feature Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sentinel_X_raw = MASTER_DATA.select_dtypes(include=[np.number])
sentinel_cols_drop = ['target', 'id', 'unnamed: 0', 'patient_id']
sentinel_X = sentinel_X_raw.drop(columns=[c for c in sentinel_cols_drop if c in sentinel_X_raw.columns])
sentinel_y = MASTER_DATA['target'].fillna(0).astype(int)

# â”€â”€ 2. Independent Stratified Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sentinel_X_train, sentinel_X_test, sentinel_y_train, sentinel_y_test = train_test_split(
    sentinel_X, sentinel_y, test_size=0.2, random_state=42, stratify=sentinel_y,
)

# â”€â”€ 3. Independent Pipeline (Imputer â†’ MinMaxScaler â†’ NB) â”€â”€â”€â”€
sentinel_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler',  MinMaxScaler()),
    ('clf',     GaussianNB()),
])

# â”€â”€ 4. Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t0 = time.time()
sentinel_pipeline.fit(sentinel_X_train, sentinel_y_train)
sentinel_elapsed = time.time() - t0

# â”€â”€ 5. Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sentinel_y_pred = sentinel_pipeline.predict(sentinel_X_test)
sentinel_y_prob = sentinel_pipeline.predict_proba(sentinel_X_test)[:, 1]
sentinel_acc = accuracy_score(sentinel_y_test, sentinel_y_pred)
sentinel_auc = roc_auc_score(sentinel_y_test, sentinel_y_prob)

# â”€â”€ 6. Store in Tournament â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tournament_results.append({
    'Model': 'Sentinel Node (NB)',
    'Accuracy': sentinel_acc,
    'ROC-AUC': sentinel_auc,
    'Train Time (s)': round(sentinel_elapsed, 2),
})
tournament_predictions['Sentinel Node\n(Naive Bayes)'] = sentinel_y_pred
tournament_probabilities['Sentinel Node (NB)'] = sentinel_y_prob

print(f"  âœ“ Sentinel Node (NB)             Acc={sentinel_acc:.4f}  AUC={sentinel_auc:.4f}  ({sentinel_elapsed:.1f}s)")
print(f"    Dataset: {sentinel_X.shape[0]:,} patients Ã— {sentinel_X.shape[1]} features")
print(f"    Train: {len(sentinel_X_train):,}  Test: {len(sentinel_X_test):,}")
print("âœ… Sentinel Node â€” Independent pipeline complete (3/5)")

"""### ğŸ›¡ï¸ Vanguard System â€” Logistic Regression (4/5)

| Property | Detail |
|---|---|
| **Purpose** | Establishing a linear decision boundary to test for simple linear relationships in the data |
| **Pipeline Architecture** | `SimpleImputer` (Median) â†’ `StandardScaler` â†’ `LogisticRegression` |
| **Key Hyperparameters** | `max_iter=1000` (extended convergence time for stability), `random_state=42` |
| **Interpretability** | Highly interpretable via coefficients (odds ratios), serving as a transparent benchmark for the "Black Box" models |
| **Independence** | Maintains complete isolation with its own `train_test_split` and feature selection step |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  VANGUARD SYSTEM â€” Independent Logistic Regression Pipeline (4/5)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Independent Feature Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vanguard_X_raw = MASTER_DATA.select_dtypes(include=[np.number])
vanguard_cols_drop = ['target', 'id', 'unnamed: 0', 'patient_id']
vanguard_X = vanguard_X_raw.drop(columns=[c for c in vanguard_cols_drop if c in vanguard_X_raw.columns])
vanguard_y = MASTER_DATA['target'].fillna(0).astype(int)

# â”€â”€ 2. Independent Stratified Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vanguard_X_train, vanguard_X_test, vanguard_y_train, vanguard_y_test = train_test_split(
    vanguard_X, vanguard_y, test_size=0.2, random_state=42, stratify=vanguard_y,
)

# â”€â”€ 3. Independent Pipeline (Imputer â†’ Scaler â†’ LogReg) â”€â”€â”€â”€â”€â”€
vanguard_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler',  StandardScaler()),
    ('clf',     LogisticRegression(max_iter=1000, random_state=42)),
])

# â”€â”€ 4. Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t0 = time.time()
vanguard_pipeline.fit(vanguard_X_train, vanguard_y_train)
vanguard_elapsed = time.time() - t0

# â”€â”€ 5. Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vanguard_y_pred = vanguard_pipeline.predict(vanguard_X_test)
vanguard_y_prob = vanguard_pipeline.predict_proba(vanguard_X_test)[:, 1]
vanguard_acc = accuracy_score(vanguard_y_test, vanguard_y_pred)
vanguard_auc = roc_auc_score(vanguard_y_test, vanguard_y_prob)

# â”€â”€ 6. Store in Tournament â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tournament_results.append({
    'Model': 'Vanguard System (LogReg)',
    'Accuracy': vanguard_acc,
    'ROC-AUC': vanguard_auc,
    'Train Time (s)': round(vanguard_elapsed, 2),
})
tournament_predictions['Vanguard System\n(Logistic Regression)'] = vanguard_y_pred
tournament_probabilities['Vanguard System (LogReg)'] = vanguard_y_prob

print(f"  âœ“ Vanguard System (LogReg)       Acc={vanguard_acc:.4f}  AUC={vanguard_auc:.4f}  ({vanguard_elapsed:.1f}s)")
print(f"    Dataset: {vanguard_X.shape[0]:,} patients Ã— {vanguard_X.shape[1]} features")
print(f"    Train: {len(vanguard_X_train):,}  Test: {len(vanguard_X_test):,}")
print("âœ… Vanguard System â€” Independent pipeline complete (4/5)")

"""### ğŸ’“ Pulse-Sync Architecture â€” Deep Learning CNN (5/5)

| Property | Detail |
|---|---|
| **Purpose** | Capturing non-linear, complex patterns using a 1D Convolutional Neural Network (Deep Learning) |
| **Input Strategy** | **Coupled Preprocessing:** Reuses the fitted `SimpleImputer` and `StandardScaler` from the Myo-Core pipeline to ensure consistent data scaling |
| **Data Reshaping** | Transforms 2D tabular data into a 3D Tensor `(Samples, Features, 1)` to simulate a "signal" for the Conv1D layers |
| **Architecture** |  `Conv1D(64)` â†’ `Conv1D(32)` â†’ `Flatten` â†’ `Dense(64)` â†’ `Dropout(0.3)` â†’ `Output(Sigmoid)` |
| **Training** | 10 Epochs, Batch Size 64, `Adam` Optimizer, `Binary Crossentropy` Loss |
| **Role** | The "Wildcard" entry â€” uses Deep Learning to find feature interactions that traditional tree-based models might miss |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PULSE-SYNC ARCHITECTURE â€” Deep Learning CNN (Contestant 5)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# â”€â”€ Preprocess using the same imputer/scaler from Myo-Core â”€â”€â”€â”€â”€
X_train_nn = myocore_pipeline[:-1].transform(X_train_raw)
X_test_nn  = myocore_pipeline[:-1].transform(X_test_raw)

# â”€â”€ Reshape to 3-D for Conv1D: (samples, features, 1) â”€â”€â”€â”€â”€â”€â”€â”€â”€
n_features = X_train_nn.shape[1]
X_train_3d = X_train_nn.reshape(-1, n_features, 1)
X_test_3d  = X_test_nn.reshape(-1, n_features, 1)

# â”€â”€ Build Pulse-Sync CNN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pulse_sync = keras.Sequential([
    layers.Conv1D(filters=64, kernel_size=3, activation='relu',
                  padding='same', input_shape=(n_features, 1)),
    layers.Conv1D(filters=32, kernel_size=3, activation='relu',
                  padding='same'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid'),
], name='Pulse_Sync_CNN')

pulse_sync.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy'],
)

print("âš¡ Pulse-Sync Architecture Summary:")
pulse_sync.summary()

# â”€â”€ Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t0 = time.time()
history = pulse_sync.fit(
    X_train_3d, y_train,
    epochs=10,
    batch_size=64,
    validation_split=0.15,
    verbose=1,
)
elapsed_cnn = time.time() - t0

# â”€â”€ Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
y_prob_cnn = pulse_sync.predict(X_test_3d, verbose=0).ravel()
y_pred_cnn = (y_prob_cnn >= 0.5).astype(int)

acc_cnn = accuracy_score(y_test, y_pred_cnn)
auc_cnn = roc_auc_score(y_test, y_prob_cnn)

print(f"\n  âœ“ Pulse-Sync CNN               Acc={acc_cnn:.4f}  AUC={auc_cnn:.4f}  ({elapsed_cnn:.1f}s)")

# Append to tournament results
results.append({
    'Model': 'Pulse-Sync (CNN)',
    'Accuracy': acc_cnn,
    'ROC-AUC': auc_cnn,
    'Train Time (s)': round(elapsed_cnn, 2),
})

# Store in model_zoo for reference (not a Pipeline, but kept for parity)
model_zoo['Pulse-Sync (CNN)'] = pulse_sync
print("\nâœ… 5 / 5 contestants trained.")

"""### ğŸ’“ Pulse-Sync Architecture â€” Deep Learning CNN (5/5)

| Property | Detail |
|---|---|
| **Purpose** | Capturing non-linear, complex patterns using a 1D Convolutional Neural Network (Deep Learning) |
| **Input Shape** | 3D Tensor: `(Samples, Features, 1)` â€” treating patient features as a "signal" sequence |
| **Preprocessing** | **Independent** `SimpleImputer` (Median) and `StandardScaler` to ensure neural network stability without data leakage |
| **Architecture** |  `Conv1D(64)` â†’ `Conv1D(32)` â†’ `Flatten` â†’ `Dense(64)` â†’ `Dropout(0.3)` â†’ `Output(Sigmoid)` |
| **Training** | 10 Epochs, Batch Size 64, `Adam` Optimizer, `Binary Crossentropy` Loss |
| **Independence** | Fully isolated pipeline (does not rely on previous models) to ensure a fair "Tournament" comparison |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PULSE-SYNC â€” Independent 1D-CNN Deep Learning Pipeline (5/5)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Independent Feature Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pulse_X_raw = MASTER_DATA.select_dtypes(include=[np.number])
pulse_cols_drop = ['target', 'id', 'unnamed: 0', 'patient_id']
pulse_X = pulse_X_raw.drop(columns=[c for c in pulse_cols_drop if c in pulse_X_raw.columns])
pulse_y = MASTER_DATA['target'].fillna(0).astype(int)

# â”€â”€ 2. Independent Stratified Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pulse_X_train_raw, pulse_X_test_raw, pulse_y_train, pulse_y_test = train_test_split(
    pulse_X, pulse_y, test_size=0.2, random_state=42, stratify=pulse_y,
)

# â”€â”€ 3. Imputer & Scaler (fit only on train) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pulse_imputer = SimpleImputer(strategy='median')
pulse_scaler  = StandardScaler()
pulse_X_train_imp = pulse_imputer.fit_transform(pulse_X_train_raw)
pulse_X_test_imp  = pulse_imputer.transform(pulse_X_test_raw)
pulse_X_train = pulse_scaler.fit_transform(pulse_X_train_imp)
pulse_X_test  = pulse_scaler.transform(pulse_X_test_imp)

# â”€â”€ 4. Reshape for Conv1D â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
n_features = pulse_X_train.shape[1]
pulse_X_train_3d = pulse_X_train.reshape(-1, n_features, 1)
pulse_X_test_3d  = pulse_X_test.reshape(-1, n_features, 1)

# â”€â”€ 5. Build & Train CNN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pulse_sync = keras.Sequential([
    layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(n_features, 1)),
    layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid'),
], name='Pulse_Sync_CNN')

pulse_sync.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

print("âš¡ Pulse-Sync Architecture Summary:")
pulse_sync.summary()

# â”€â”€ 6. Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t0 = time.time()
history = pulse_sync.fit(
    pulse_X_train_3d, pulse_y_train,
    epochs=10,
    batch_size=64,
    validation_split=0.15,
    verbose=1,
)
pulse_elapsed = time.time() - t0

# â”€â”€ 7. Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pulse_y_prob = pulse_sync.predict(pulse_X_test_3d, verbose=0).ravel()
pulse_y_pred = (pulse_y_prob >= 0.5).astype(int)
pulse_acc = accuracy_score(pulse_y_test, pulse_y_pred)
pulse_auc = roc_auc_score(pulse_y_test, pulse_y_prob)

# â”€â”€ 8. Store in Tournament â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tournament_results.append({
    'Model': 'Pulse-Sync (CNN)',
    'Accuracy': pulse_acc,
    'ROC-AUC': pulse_auc,
    'Train Time (s)': round(pulse_elapsed, 2),
})
tournament_predictions['Pulse-Sync\n(CNN)'] = pulse_y_pred
tournament_probabilities['Pulse-Sync (CNN)'] = pulse_y_prob

print(f"  âœ“ Pulse-Sync (CNN)               Acc={pulse_acc:.4f}  AUC={pulse_auc:.4f}  ({pulse_elapsed:.1f}s)")
print(f"    Dataset: {pulse_X.shape[0]:,} patients Ã— {pulse_X.shape[1]} features")
print(f"    Train: {len(pulse_X_train):,}  Test: {len(pulse_X_test):,}")
print("âœ… Pulse-Sync â€” Independent pipeline complete (5/5)")

"""### ğŸ† Tournament Leaderboard â€” Final Standings

| Property | Detail |
|---|---|
| **Purpose** | Aggregate and rank all model performance metrics to declare a winner |
| **Ranking Metric** | `ROC-AUC` (Area Under the Receiver Operating Characteristic Curve) is used as the primary sorting metric |
| **Output** | A formatted text table showing Rank, Model Name, Accuracy, ROC-AUC, and Training Time |
| **Winner Selection** | The model with the highest ROC-AUC is automatically crowned as the "Champion" |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ğŸ† TOURNAMENT LEADERBOARD â€” Results Table Only
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import pandas as pd

# Results Table
leaderboard_df = pd.DataFrame(tournament_results).sort_values('ROC-AUC', ascending=False)
leaderboard_df.index = range(1, len(leaderboard_df) + 1)
leaderboard_df.index.name = 'Rank'

print("=" * 62)
print("         ğŸ†  MYO AI â€” MODEL TOURNAMENT RESULTS  ğŸ†")
print("=" * 62)
print(leaderboard_df.to_string())
print("=" * 62)

# Crown the winner
winner = leaderboard_df.iloc[0]
print(f"\nğŸ‘‘ Tournament Champion: {winner['Model']}")
print(f"   Accuracy : {winner['Accuracy']:.4f}")
print(f"   ROC-AUC  : {winner['ROC-AUC']:.4f}")

"""### ğŸ“Š Confusion Matrix Grid â€” Tournament Visualization

| Property | Detail |
|---|---|
| **Purpose** | Visually compare the classification errors (False Positives vs. False Negatives) across all 5 models simultaneously |
| **Visual Format** | **2x3 Grid of Heatmaps** â€” each subplot represents one model's confusion matrix |
| **Key Insight** | The **Diagonal** (Top-Left & Bottom-Right) shows correct predictions (Healthy & CVD). The **Off-Diagonal** shows errors (Missed Cases vs. False Alarms) |
| **Input Data** | Iterates through the `tournament_predictions` dictionary to fetch pre-calculated prediction vectors |
| **Styling** | Uses `Seaborn` heatmaps with annotation (`annot=True`) to show raw patient counts in each quadrant |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  CONFUSION MATRIX GRID â€” All 6 Models
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

plt.style.use('seaborn-v0_8-darkgrid')

# Include all models for visualization
model_names_to_plot = list(tournament_predictions.keys())

num_models = len(model_names_to_plot)
rows = 2
cols = 3

fig, axes = plt.subplots(rows, cols, figsize=(18, 11))
axes_flat = axes.flatten()

for idx, name in enumerate(model_names_to_plot):
    y_pred = tournament_predictions[name]
    cm = confusion_matrix(tournament_y_test, y_pred)
    ax = axes_flat[idx]
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                cbar=False, linewidths=0.8, linecolor='white',
                annot_kws={'size': 16, 'weight': 'bold'})
    ax.set_title(name, fontsize=12, fontweight='bold', pad=8)
    ax.set_ylabel('Actual' if idx % cols == 0 else '', fontsize=11)
    ax.set_xlabel('Predicted', fontsize=11)
    ax.set_xticklabels(['Healthy', 'CVD'], fontsize=9)
    ax.set_yticklabels(['Healthy', 'CVD'], fontsize=9, rotation=0)

# Hide any unused subplots (if num_models < rows * cols - this case is fine, all will be used)
for i in range(num_models, len(axes_flat)):
    axes_flat[i].axis('off')
    for text_obj in axes_flat[i].texts:
        text_obj.remove()

fig.suptitle('Myo AI: Confusion Matrix Grid â€” All 6 Contestants',
             fontsize=16, fontweight='bold', y=1.01)
plt.tight_layout()
plt.show()

"""### ğŸ“ˆ Combined ROC Curve â€” Tournament Visualization

| Property | Detail |
|---|---|
| **Purpose** | Compare the trade-off between Sensitivity (TPR) and False Alarm Rate (FPR) across all 5 models on a single chart |
| **Key Metric** | `AUC` (Area Under Curve) â€” higher curves (closer to top-left) indicate better performance |
| **Visual Styling** | Distinct colors and line styles (solid, dashed, dot-dash) to differentiate between the 5 models |
| **Baseline** | Includes a diagonal dashed line representing a "Random Classifier" (AUC = 0.5) for reference |
| **Input** | Uses stored probability predictions (`y_prob`) to calculate the curve at all possible classification thresholds |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  COMBINED ROC CURVE â€” All 5 Models
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

plt.style.use('seaborn-v0_8-darkgrid')

roc_styles = [
    ('Aegis Protocol (RF)',       '#3498db', '-',  2.2),
    ('Myo-Core Engine (HGBC)',    '#2ecc71', '-',  2.8),
    ('Sentinel Node (NB)',        '#e67e22', '--', 2.2),
    ('Vanguard System (LogReg)',  '#9b59b6', '--', 2.2),
    ('Pulse-Sync (CNN)',          '#e74c3c', '-.', 2.2),
]

fig, ax = plt.subplots(figsize=(10, 8))

for (name, color, ls, lw) in roc_styles:
    y_prob = tournament_probabilities.get(name)
    if y_prob is not None:
        fpr, tpr, _ = roc_curve(tournament_y_test, y_prob)
        auc_val = auc(fpr, tpr)
        ax.plot(fpr, tpr, color=color, linestyle=ls, linewidth=lw,
                label=f'{name}  (AUC = {auc_val:.4f})')

# Diagonal reference
ax.plot([0, 1], [0, 1], 'k--', alpha=0.4, linewidth=1, label='Random Classifier')

ax.set_xlabel('False Positive Rate', fontsize=13)
ax.set_ylabel('True Positive Rate', fontsize=13)
ax.set_title('Myo AI: Combined ROC Curves â€” 5-Model Tournament',
             fontsize=16, fontweight='bold')
ax.legend(loc='lower right', fontsize=11, framealpha=0.9)
ax.set_xlim([-0.01, 1.01])
ax.set_ylim([-0.01, 1.01])
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

"""# ğŸ§  LAYER 3 â€” THE INTELLIGENCE (Analysis & UI)

> Unsupervised clustering, explainability, and interactive risk simulation. Each cell is documented and contains only one type of output (table, plot, or widget).

| # | Module | Technique | Purpose |
|:---:|---|---|---|
| 1 | **Zenith** | PCA + KMeans | Unsupervised patient risk-group clustering |
| 2 | **Oracle** | SHAP TreeExplainer, Permutation Importance | Global feature-impact explainability |
| 3 | **Myo-Sim** | ipywidgets + Chronos | Interactive risk simulator with 20-year projection |

### ğŸŒŒ Zenith Cluster Map â€” Unsupervised Patient Grouping

| Property | Detail |
|---|---|
| **Purpose** | Unsupervised discovery of patient "Risk Phenotypes" without using the target labels |
| **Input Data** | `myocore_X_test` â€” Uses the **scaled features** from the Myo-Core engine to ensure valid PCA results |
| **Methodology** | **PCA** (Principal Component Analysis) reduces high-dimensional data to 2D â†’ **K-Means Clustering** groups patients by similarity |
| **Risk Assignment** | Clusters are sorted by their PC1 value to automatically assign "Low", "Moderate", or "High" risk labels |
| **Visualization** | A 2D Scatter Plot showing patient distribution, where colors represent risk groups and "X" markers indicate cluster centroids |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ZENITH CLUSTER MAP â€” Unsupervised Patient Risk Grouping
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# 1. Reduce to 2-D with PCA
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(myocore_X_test)

print(f"PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}")

# 2. KMeans clustering (3 patient risk groups)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_pca)

# 3. Risk-group labelling (order clusters by mean PC1 value)
cluster_order = [X_pca[clusters == c, 0].mean() for c in range(3)]
label_map = {c: rank for rank, c in enumerate(sorted(range(3), key=lambda i: cluster_order[i]))}
cluster_labels = [label_map[c] for c in clusters]

risk_names  = {0: 'Low Risk', 1: 'Moderate Risk', 2: 'High Risk'}
risk_colors = {0: '#2ecc71',  1: '#f39c12',       2: '#e74c3c'}

# 4. Scatter plot
fig, ax = plt.subplots(figsize=(10, 7))
for group_id in range(3):
    mask = [cl == group_id for cl in cluster_labels]
    ax.scatter(
        X_pca[mask, 0], X_pca[mask, 1],
        c=risk_colors[group_id],
        label=risk_names[group_id],
        alpha=0.55, s=18, edgecolors='none',
    )

# Plot cluster centroids
centroids_pca = kmeans.cluster_centers_  # kmeans.cluster_centers_ is already in PCA space
for c in range(3):
    ax.scatter(centroids_pca[c, 0], centroids_pca[c, 1],
               marker='X', s=200, c='black', edgecolors='white', linewidths=1.5)

ax.set_xlabel('Principal Component 1', fontsize=12)
ax.set_ylabel('Principal Component 2', fontsize=12)
ax.set_title('Zenith Map: Patient Risk Clusters', fontsize=16, fontweight='bold')
ax.legend(fontsize=11, loc='upper right')
plt.tight_layout()
plt.show()

# Summary
for g in range(3):
    count = sum(1 for cl in cluster_labels if cl == g)
    print(f"  {risk_names[g]:15s} â†’ {count:,} patients")

"""### ğŸ“‰ Permutation Importance â€” Feature Robustness Check

| Property | Detail |
|---|---|
| **Purpose** | Determine which features are most critical to the Myo-Core model's decision-making |
| **Technique** | **Permutation Importance** â€” Randomly shuffles one feature at a time and measures the drop in model performance |
| **Metric** | **Mean Accuracy Decrease** â€” Higher values indicate the feature is more important (the model "breaks" without it) |
| **Robustness** | Calculated over **10 repeats** to generate error bars, showing the stability of the feature's importance |
| **Visualization** | Horizontal Bar Chart displaying the Top 10 features sorted by their impact on prediction accuracy |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PERMUTATION IMPORTANCE â€” Myo-Core Model Robustness Check
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt

plt.style.use('seaborn-v0_8-darkgrid')

# Compute permutation importance on the full Myo-Core pipeline
# using the raw (un-preprocessed) test set â€” the pipeline handles
# imputation + scaling internally before scoring.
print("Computing Permutation Importance (10 repeats)...")
perm_result = permutation_importance(
    myocore_pipeline,
    myocore_X_test_raw,
    myocore_y_test,
    n_repeats=10,
    random_state=42,
    n_jobs=-1,
    scoring='accuracy',
)

# Rank and select Top 10
sorted_idx = perm_result.importances_mean.argsort()
top10_idx  = sorted_idx[-10:]

top10_names  = [myocore_X.columns[i] for i in top10_idx]
top10_means  = perm_result.importances_mean[top10_idx]
top10_stds   = perm_result.importances_std[top10_idx]

# Horizontal bar chart with error bars
fig, ax = plt.subplots(figsize=(10, 7))
bars = ax.barh(
    range(len(top10_idx)),
    top10_means,
    xerr=top10_stds,
    align='center',
    color='#2ecc71',
    edgecolor='white',
    linewidth=0.6,
    capsize=4,
    error_kw={'elinewidth': 1.5, 'capthick': 1.5, 'color': '#2c3e50'},
)

ax.set_yticks(range(len(top10_idx)))
ax.set_yticklabels(top10_names, fontsize=11)
ax.set_xlabel('Mean Accuracy Decrease', fontsize=12)
ax.set_title('Myo-Core: Permutation Importance (Model Robustness Check)',
             fontsize=14, fontweight='bold')

# Annotate values on bars
for i, (mean, std) in enumerate(zip(top10_means, top10_stds)):
    ax.text(mean + std + 0.001, i, f'{mean:.4f}', va='center', fontsize=9)

plt.tight_layout()
plt.show()

print("âœ… Permutation Importance Complete.")

"""### ğŸ”® Oracle Layer â€” SHAP Explainability Suite

| Visualization | Purpose | Interpretation |
|---|---|---|
| **1. Beeswarm Plot** | **Global Overview** | Shows the top risk factors across *all* patients. <br>â€¢ **Red/Blue Color:** High/Low feature value. <br>â€¢ **X-Axis:** Impact on risk (Right = Higher Risk). |
| **2. Force Plot** | **Local "Tug-of-War"** | Visualizes the conflict between risk factors for a *single* patient. <br>â€¢ **Red Bars:** Features pushing risk **UP**. <br>â€¢ **Blue Bars:** Features pushing risk **DOWN**. |
| **3. Waterfall Plot** | **Decision Logic** | A step-by-step breakdown of how the model reached a specific prediction. <br>â€¢ Starts at the **Base Value** (Average Risk) and adds/subtracts values until the final score is reached. |

### ğŸ”® Oracle Layer â€” SHAP Beeswarm Plot

| Property | Detail |
|---|---|
| **Purpose** | Explain *how* and *why* the model makes decisions for individual patients (Global Interpretability) |
| **Technique** | **SHAP (SHapley Additive exPlanations)** â€” A game-theoretic approach to feature attribution |
| **Visualization** | **Beeswarm Plot** â€” Shows the distribution of SHAP values for each feature |
| **Interpretation** | **Color:** Feature Value (Red = High, Blue = Low) <br> **X-Axis:** Impact on Model Output (Right = Drives Risk Up, Left = Drives Risk Down) |
| **Example** | If "High Blood Pressure" (Red dots) is on the right side, it means high BP increases CVD risk |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ORACLE LAYER â€” SHAP Beeswarm Summary Plot (Global View)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import shap
import matplotlib.pyplot as plt

# 1. Initialize Explainer
explainer = shap.TreeExplainer(myocore_model)
n_explain = min(300, myocore_X_test.shape[0])
X_explain = myocore_X_test[:n_explain]

# 2. Calculate SHAP values
shap_values = explainer.shap_values(X_explain)

print("Oracle Layer: Computing feature-level SHAP impact...")
plt.figure(figsize=(12, 8))

# 3. Draw Beeswarm
shap.summary_plot(
    shap_values,
    X_explain,
    feature_names=myocore_feature_names,
    plot_type="dot",
    show=False,
)

plt.title("Oracle Layer: Multimodal Feature Impact (SHAP Analysis)",
          fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print("âœ… Oracle Layer Beeswarm Complete.")

"""### âš”ï¸ Oracle Layer â€” SHAP Force Plot

| Property | Detail |
|---|---|
| **Purpose** | Visualize the "tug-of-war" between conflicting features for a *single patient's* prediction |
| **Technique** | **Additive Feature Attribution** â€” Summing positive and negative vectors to reach a final score |
| **Visualization** | **Force Bar** â€” A horizontal bar chart where length equals impact magnitude |
| **Interpretation** | **Red Bars:** Forces pushing the prediction **HIGHER** (towards disease) <br> **Blue Bars:** Forces pushing the prediction **LOWER** (towards health) |
| **Example** | A patient might have high age (Red push) but low cholesterol (Blue push), balancing out their risk |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ORACLE LAYER â€” SHAP Force Plot (Single Patient Analysis)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Select a high-risk patient (Highest predicted probability in our slice)
# We sum the SHAP values to find the patient where the model pushes "Risk" the hardest
risk_scores = shap_values.sum(axis=1)
patient_idx = risk_scores.argmax()

print(f"ğŸ” Analyzing High-Risk Patient at Index: {patient_idx}")

# 2. Draw Force Plot
# Note: matplotlib=True allows it to render as a static image in the notebook
plt.figure(figsize=(20, 4))
shap.force_plot(
    explainer.expected_value,
    shap_values[patient_idx],
    X_explain[patient_idx], # Corrected: Changed .iloc to direct indexing
    feature_names=myocore_feature_names,
    matplotlib=True,
    show=False
)

plt.title(f"Force Plot: Feature 'Tug-of-War' for Patient #{patient_idx}",
          fontsize=14, fontweight='bold', y=1.5)
plt.tight_layout()
plt.show()

print("âœ… Oracle Layer Force Plot Complete.")

"""### ğŸŒŠ Oracle Layer â€” SHAP Waterfall Plot

| Property | Detail |
|---|---|
| **Purpose** | A step-by-step audit trail showing how the model moved from the "Average" to the "Specific" |
| **Technique** | **Sequential Breakdown** â€” Starts at the baseline probability and adds/subtracts each feature's contribution |
| **Visualization** | **Staircase Chart** â€” Each step represents a feature adding to or subtracting from the total risk |
| **Interpretation** | **$E[f(x)]$ (Base Value):** The average risk for the whole population <br> **$f(x)$ (Final Output):** The specific predicted risk for this patient |
| **Example** | Start at 15% risk â†’ Age adds +5% â†’ Smoking adds +10% â†’ Exercise subtracts -3% â†’ Final Prediction = 22% |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ORACLE LAYER â€” SHAP Waterfall Plot (Step-by-Step Logic)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Create a SHAP Explanation Object (Required for Waterfall plots)
# This packages the data, values, and base value together
shap_explanation = shap.Explanation(
    values=shap_values[patient_idx],
    base_values=explainer.expected_value,
    data=X_explain[patient_idx], # Corrected: Changed .iloc to direct indexing
    feature_names=myocore_feature_names
)

# 2. Draw Waterfall Plot
plt.figure(figsize=(8, 8)) # Vertical plots need standard aspect ratio
shap.plots.waterfall(
    shap_explanation,
    max_display=12,
    show=False
)

plt.title(f"Waterfall Plot: Decision Path for Patient #{patient_idx}",
          fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("âœ… Oracle Layer Waterfall Plot Complete.")

"""### ğŸ“Š Oracle Layer â€” SHAP Global Importance (Bar Plot)

| Property | Detail |
|---|---|
| **Purpose** | Rank features based on their average absolute impact on the model's predictions |
| **Metric** | **Mean |SHAP| Value** â€” The average magnitude of the feature's contribution (ignoring direction) |
| **Visualization** | **Bar Chart** â€” Longer bars indicate features that consistently push the model's prediction (either up or down) the most |
| **Difference** | Unlike the Beeswarm plot (which shows *how* a feature affects risk), this plot simply shows *how much* it matters overall |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ORACLE LAYER â€” SHAP Bar Plot (Myo-Core)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
shap.summary_plot(
    shap_values,
    X_explain,
    feature_names=myocore_feature_names,
    plot_type="bar",
    show=False,
)
plt.title("Oracle Layer: Mean |SHAP| Feature Importance",
          fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("âœ… Oracle Layer Bar Plot Complete.")

"""### ğŸ§¬ Myo-Sim Bio-Deck â€” Chronos Time-Travel Dashboard

| Property | Detail |
|---|---|
| **Purpose** | Interactive digital twin for patient risk simulation and 20-year projection |
| **Input** | User widget controls (age, BP, cholesterol, weight, height, smoker, active, years ahead) |
| **Output** | Gauge chart (current risk), line plot (20-year risk projection), stats panel |
| **Design** | Uses `ipywidgets`, `matplotlib`, and Myo-Core's fitted pipeline for real-time inference |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MYO-SIM BIO-DECK â€” Interactive "Chronos" Time-Travel Dashboard
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import ipywidgets as widgets
from IPython.display import display, clear_output
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import numpy as np
import pandas as pd


def _predict_risk(age, sys_bp, dia_bp, cholesterol, weight, height,
                  smoker, active, years_future=0):
    """
    Build a single-patient feature vector matching the Myo-Core
    pipeline's training schema, then return P(CVD).
    """
    sim_age = age + years_future
    bmi = weight / ((height / 100) ** 2) if height > 0 else 0
    pulse_pressure = sys_bp - dia_bp

    # Start from zeros matching model feature order
    feature_names = myocore_X.columns.tolist()
    patient = {f: 0.0 for f in feature_names}

    # Map widget inputs â†’ canonical feature names
    mapping = {
        'age': sim_age,
        'ap_hi': sys_bp, 'sys_bp': sys_bp, 'restingbp': sys_bp,
        'ap_lo': dia_bp,
        'cholesterol': cholesterol,
        'weight': weight,
        'height': height,
        'bmi': bmi,
        'pulse_pressure': pulse_pressure,
        'smoke': int(smoker),
        'active': int(active),
        'sensor_signal_available': 0,
    }
    for key, val in mapping.items():
        if key in patient:
            patient[key] = val

    df_patient = pd.DataFrame([patient], columns=feature_names)
    df_imputed = myocore_imputer.transform(df_patient)
    df_scaled  = myocore_scaler.transform(df_imputed)
    prob = myocore_model.predict_proba(df_scaled)[:, 1][0]
    return prob


def _draw_gauge(ax, prob):
    """Render a semicircular gauge chart for CVD risk probability."""
    ax.clear()
    ax.set_aspect('equal')
    ax.set_xlim(-1.3, 1.3)
    ax.set_ylim(-0.3, 1.4)
    ax.axis('off')

    # Background arc segments (green â†’ yellow â†’ red)
    n_seg = 100
    for i in range(n_seg):
        frac = i / n_seg
        angle = np.pi * (1 - frac)  # pi â†’ 0
        if frac < 0.4:
            color = '#2ecc71'
        elif frac < 0.7:
            color = '#f39c12'
        else:
            color = '#e74c3c'
        a1 = np.degrees(np.pi - frac * np.pi)
        a2 = np.degrees(np.pi - (frac + 1/n_seg) * np.pi)
        wedge = mpatches.Wedge((0, 0), 1.0, min(a1, a2), max(a1, a2),
                               width=0.25, color=color, alpha=0.35)
        ax.add_patch(wedge)

    # Needle
    needle_angle = np.pi * (1 - prob)
    nx = 0.85 * np.cos(needle_angle)
    ny = 0.85 * np.sin(needle_angle)
    ax.annotate('', xy=(nx, ny), xytext=(0, 0),
                arrowprops=dict(arrowstyle='->', color='black', lw=2.5))
    ax.plot(0, 0, 'ko', markersize=8)

    # Labels
    status = 'HIGH RISK' if prob > 0.5 else 'LOW RISK'
    status_color = '#e74c3c' if prob > 0.5 else '#2ecc71'
    ax.text(0, -0.15, f'{prob:.1%}', ha='center', va='center',
            fontsize=28, fontweight='bold', color=status_color)
    ax.text(0, 1.25, 'CVD Risk Gauge', ha='center', va='center',
            fontsize=14, fontweight='bold')
    ax.text(-1.1, -0.05, '0%', fontsize=9, ha='center')
    ax.text(1.1, -0.05, '100%', fontsize=9, ha='center')
    ax.text(0, -0.30, status, ha='center', fontsize=13,
            fontweight='bold', color=status_color,
            bbox=dict(boxstyle='round,pad=0.3', facecolor=status_color,
                      alpha=0.15, edgecolor=status_color))


def _draw_chronos_projection(ax, age, sys_bp, dia_bp, cholesterol,
                              weight, height, smoker, active):
    """Plot projected CVD risk over the next 20 years."""
    ax.clear()
    years = list(range(0, 21))
    risks = [
        _predict_risk(age, sys_bp, dia_bp, cholesterol, weight, height,
                      smoker, active, y)
        for y in years
    ]
    ages = [age + y for y in years]

    ax.fill_between(ages, risks, alpha=0.15, color='#e74c3c')
    ax.plot(ages, risks, 'o-', color='#e74c3c', linewidth=2.5,
            markersize=5, label='Projected CVD Risk')
    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.6, label='Risk Threshold (50%)')

    ax.set_xlabel('Age (years)', fontsize=12)
    ax.set_ylabel('CVD Probability', fontsize=12)
    ax.set_title('Chronos Engine: 20-Year Risk Projection', fontsize=14, fontweight='bold')
    ax.set_ylim(-0.02, 1.02)
    ax.legend(loc='upper left', fontsize=10)
    ax.grid(True, alpha=0.3)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  WIDGET LAYOUT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

style = {'description_width': '120px'}
layout = widgets.Layout(width='340px')

w_age         = widgets.IntSlider(value=45, min=18, max=100, step=1,
                                  description='Age:', style=style, layout=layout)
w_sys_bp      = widgets.IntSlider(value=130, min=80, max=220, step=1,
                                  description='Systolic BP:', style=style, layout=layout)
w_dia_bp      = widgets.IntSlider(value=80, min=40, max=130, step=1,
                                  description='Diastolic BP:', style=style, layout=layout)
w_cholesterol = widgets.IntSlider(value=200, min=50, max=600, step=5,
                                  description='Cholesterol:', style=style, layout=layout)
w_weight      = widgets.FloatSlider(value=75, min=30, max=200, step=0.5,
                                    description='Weight (kg):', style=style, layout=layout)
w_height      = widgets.FloatSlider(value=170, min=100, max=220, step=0.5,
                                    description='Height (cm):', style=style, layout=layout)
w_smoker      = widgets.ToggleButton(value=False, description='Smoker',
                                     button_style='', icon='smoking-ban',
                                     layout=widgets.Layout(width='160px'))
w_active      = widgets.ToggleButton(value=True, description='Active',
                                     button_style='success', icon='heartbeat',
                                     layout=widgets.Layout(width='160px'))
w_years       = widgets.IntSlider(value=0, min=0, max=20, step=1,
                                  description='â³ Years Ahead:', style=style, layout=layout)

output_area = widgets.Output()


def _on_change(*args):
    with output_area:
        clear_output(wait=True)

        age   = w_age.value
        sys_  = w_sys_bp.value
        dia_  = w_dia_bp.value
        chol  = w_cholesterol.value
        wt    = w_weight.value
        ht    = w_height.value
        smoke = w_smoker.value
        act   = w_active.value
        yrs   = w_years.value

        # Current risk (at slider year offset)
        prob = _predict_risk(age, sys_, dia_, chol, wt, ht, smoke, act, yrs)

        # Derived stats
        bmi = wt / ((ht / 100) ** 2) if ht > 0 else 0
        pp  = sys_ - dia_

        # â•â• Dashboard figure â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        fig = plt.figure(figsize=(15, 5.5))
        gs = fig.add_gridspec(1, 3, width_ratios=[1, 0.05, 1.6])

        ax_gauge = fig.add_subplot(gs[0, 0])
        ax_line  = fig.add_subplot(gs[0, 2])

        _draw_gauge(ax_gauge, prob)
        _draw_chronos_projection(ax_line, age, sys_, dia_, chol, wt, ht, smoke, act)

        # Highlight current year marker on projection
        ax_line.axvline(x=age + yrs, color='#3498db', linestyle='-', lw=2, alpha=0.7)
        ax_line.scatter([age + yrs], [prob], color='#3498db', s=120, zorder=5,
                        edgecolors='white', linewidths=2)
        ax_line.annotate(f'Now +{yrs}yr\n{prob:.1%}', xy=(age + yrs, prob),
                         xytext=(10, 15), textcoords='offset points',
                         fontsize=9, fontweight='bold', color='#3498db',
                         arrowprops=dict(arrowstyle='->', color='#3498db'))

        plt.tight_layout()
        plt.show()

        # â•â• Stats panel â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print("â”€" * 52)
        print(f"  Simulated Age     : {age + yrs}")
        print(f"  BMI               : {bmi:.1f}")
        print(f"  Pulse Pressure    : {pp} mmHg")
        print(f"  CVD Probability   : {prob:.2%}")
        print(f"  Status            : {'â–ˆâ–ˆ HIGH RISK' if prob > 0.5 else 'â–ˆâ–ˆ LOW RISK'}")
        print("â”€" * 52)


# â•â• Wire up all widgets to the callback â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
for w in [w_age, w_sys_bp, w_dia_bp, w_cholesterol, w_weight,
          w_height, w_smoker, w_active, w_years]:
    w.observe(_on_change, names='value')

# â•â• Build layout â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
title_html = widgets.HTML(
    "<h2 style='text-align:center; color:#FFFFFF;'>"
    " â–ˆâ–ˆ Myo-Sim Bio-Deck â€” Chronos Time-Travel Interface</h2>"
)

left_col = widgets.VBox([
    widgets.HTML("<b>Patient Vitals</b>"),
    w_age, w_sys_bp, w_dia_bp, w_cholesterol, w_weight, w_height,
    widgets.HTML("<b>Lifestyle</b>"),
    widgets.HBox([w_smoker, w_active]),
    widgets.HTML("<b>âŒ› Chronos Engine</b>"),
    w_years,
])

dashboard = widgets.VBox([
    title_html,
    widgets.HBox([
        left_col,
        widgets.VBox([output_area], layout=widgets.Layout(width='70%')),
    ]),
])

# â•â• Initial render â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
display(dashboard)
_on_change()

print("\nâœ… Myo-Sim Bio-Deck with Chronos Engine deployed.")

"""### ğŸ’¾ Layer 4 â€” The Archive (Model Export)

| Property | Detail |
|---|---|
| **Purpose** | Serialize the winning model pipeline for deployment in external applications |
| **Format** | `myocore_pipeline.pkl` (Python Pickle format via Joblib) |
| **Contents** | 1. `SimpleImputer` (Handles missing values) <br> 2. `StandardScaler` (Normalizes data) <br> 3. `HistGradientBoostingClassifier` (The trained model) |
| **Verification** | MD5 Checksum generated to ensure file integrity during transfer |
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  LAYER 4 â€” THE ARCHIVE (Save & Export)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import joblib
import hashlib
import os
from google.colab import files

# 1. Define the filename
filename = 'myocore_pipeline_v1.pkl'

# 2. Save the full pipeline (Imputer + Scaler + Model)
#    We use the pipeline object, NOT just the model, so it handles
#    preprocessing automatically for new data.
print(f"ğŸ“¦ Archiving Myo-Core Pipeline to '{filename}'...")
joblib.dump(myocore_pipeline, filename)

# 3. Verify file size
file_size = os.path.getsize(filename) / 1024  # KB
print(f"   Size: {file_size:.2f} KB")

# 4. Generate MD5 Checksum (Digital Fingerprint)
with open(filename, "rb") as f:
    file_hash = hashlib.md5(f.read()).hexdigest()
print(f"   MD5 Checksum: {file_hash}")

# 5. Trigger Download
print("\nâ¬‡ï¸ Initiating download...")
files.download(filename)

print("\nâœ… TOURNAMENT COMPLETE. System ready for deployment.")